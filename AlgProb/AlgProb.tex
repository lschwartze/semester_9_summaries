\documentclass[a4paper, 12pt]{article}

\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb}
\usepackage[explicit]{titlesec}
\usepackage{ulem}
\usepackage[onehalfspacing]{setspace}
\usepackage{amsthm}

\theoremstyle{plain}
\newtheorem{theorem}{Satz}[section] % reset theorem numbering for each chapter

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition} % definition numbers are dependent on theorem numbers
\theoremstyle{lemma}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Bemerkung}

\theoremstyle{corollary}
\newtheorem{corollary}[theorem]{Korollar}

\theoremstyle{example}
\newtheorem{example}[theorem]{Beispiel}

\titleformat{\subsection}
{\small}{\thesubsection}{1em}{\uline{#1}}
\begin{document}
	\begin{titlepage} 
		\title{Algorithmen für schwierige Probleme}
		\clearpage\maketitle
		\thispagestyle{empty}
	\end{titlepage}
	\tableofcontents
	\newpage
	\section{3-SAT und Vertex Cover}
	\subsection{Vertex Cover}
	\begin{definition}
		Ein Vertex Cover für einen Graphen $G$ ist eine Teilmenge $C \subseteq V$ der Knoten, sodass $\forall e = (u,v) \in E$ gilt $u \in C$ oder $v \in C$. 
	\end{definition}
	\begin{definition}
		Für zwei Probleme $A$, $B$ schreiben wir $A \preceq B$, wenn man $A$ effizient auf $B$ transformieren kann.
	\end{definition}
	\begin{theorem}
		Der Greedy Algorithmus, der immer den Knoten höchsten Grades wählt, kann beliebig schlechte Ergebnisse liefern.
	\end{theorem}
	\begin{theorem}
		Der folgende Algorithmus liefert eine 2-Faktor-Approximation an das optimale Vertex Cover.
		\begin{enumerate}
			\item setze $C= \varnothing$
			\item WHILE $E \neq \varnothing$
			\item wähle eine Kante $(u,v)$
			\item setze $V = V\setminus \{u,v\}$, $E = {e = (w,x) \in E \; | \; w,x \notin \{u,v\}}$ und $C = C \cup \{u,v\}$.
		\end{enumerate}
	\end{theorem}
	\begin{theorem}
		Der folgenden Ansatz benutzt außerdem einen Parameter $k$ als Eingabe und entscheidet, ob es ein Vertex Cover der Größe $\leq k$ gibt.
		\begin{enumerate}
			\item FUNCTION VCover($G,C,l$)
			\item if $E = \varnothing$ return True
			\item if $l = k$ return False
			\item Wähle eine Kante $(u,v)$ in $G$
			\item if VCover($G\setminus \{u\}, C \cup \{u\}, l+1$)
			\item return True
			\item else return VCover($G\setminus \{v\}, C\cup \{v\}, l+1$)
		\end{enumerate}
	Die Laufzeit ist offensichtlich $2^k \mathcal{O}(n)$.
	\end{theorem}
	\subsection{SAT}
	Eine Formel $F$ ist in KNF gegeben, also als Konjunktion von Disjunktionen von Literalen. Eine Belegung ist eine Funktion, die jeder Variable einen Wert zuweist. Eine partielle Belegung weist nur einer Teilmenge aller Variablen Werte zu und lässt die restlichen undefiniert.
	\begin{lemma}
		Reduktion von 3-Färbbarkeit auf SAT.\\
		Für einen Graphen $G$ wollen wir eine Formel $F$ konstruieren. Zunächst definieren wir die Menge der Variablen von $F$ als \[VAR = \{x_v^1, x_v^2, x_v^3: \; v \in V\}\] für drei verschiedene Farben. In einer Lösung muss jeder Knoten eine Farbe bekommen, d.h wir brauchen die $\left|V\right|$ Klauseln \[(x_v^1 \lor x_v^2 \lor x_v^3)\] Außerdem dürfen zwei benachbarte Knoten nicht dieselbe Farbe bekommen, das heißt, es ergeben sich die $3\left|E\right|$ Klauseln \[(\lnot {x_v^1} \lor \lnot {x_w^1}) \land (\lnot {x_v^2} \lor \lnot {x_w^2}) \land (\lnot {x_v^3} \lor \lnot {x_w^3})\] 
	\end{lemma}
	\begin{theorem}
		Der folgende \underline{Backtracking} Algorithmus bietet einen guten Ansatz, um eine Formel $F$ auf Erfüllbarkeit zu überprüfen.
		\begin{enumerate}
			\item FUNCTION search($F, \alpha$ partielle Belegung)
			\item if $\alpha$ belegt alle Variablen: return $\alpha(F)$
			\item if $\alpha$ belegt eine Klausel mit 0: return False
			\item if search($F, \alpha0$) = True: return True
			\item else: return search($F,\alpha1$)
		\end{enumerate}
		Dieser Algorithmus kann sogar noch weiter verbessert werden, indem zuerst nach Klauseln mit nur einem Literal gesucht wird und dann diese belegt werden. Wenn diese nicht existieren, wird nach Klauseln mit zwei Literalen gesucht und diese werden belegt. Erst dann wird eine beliebige Klausel (bzw. ein beliebiges Literal) gewählt. Die Laufzeit ist beschränkt durch $\mathcal{O}(7^{\frac{n}{3}})$.
	\end{theorem}
	Das Erfüllbarkeitsproblem, in dem jede Klausel aus maximal zwei Variablen besteht, wird 2-SAT genannt. Dieses Problem kann in polynomieller Zeit gelöst werden.
	\begin{theorem}
		Im folgenden wird ein probabilistischer Algorithmus für 2-SAT eingeführt.
		\begin{enumerate}
			\item $\alpha := (0,0,...,0)$
			\item for $i = 0$ to $f(F)$
			\item if $\alpha(F) = 1$: return True
			\item wähle eine zufällige Klausel $C$ mit $\alpha(C) = 0$
			\item wähle ein zufälliges Literal $x$ aus $C$
			\item setze $\alpha(x) = \overline{\alpha(x)}$
		\end{enumerate}
		Die Funktion $f: F \mapsto n \in \mathbb{N}$ wird später definiert.\\
		Ist $F$ unerfüllbar, so funktioniert der Algorithmus korrekt. Ist $F$ erfüllbar, so findet der Algorithmus eine erfüllende Belegung mit Wahrscheinlichkeit $p \geq \frac{1}{2}$.\\
		Ist $\alpha^*$ eine erfüllende Belegung und $\alpha$ eine beliebige andere Belegung, mit Hamming-Abstand $i$ zu $\alpha^*$, so definieren wir uns $T(i)$ als die erwartete Anzahl an Bit-Flips, die nötig sind, um $\alpha$ in eine erfüllende Belegung zu transformieren. Man kann sehen, dass $T(n) = n^2$. Die erwartete Anzahl an Bitflips ist also $\mathcal{O}(n^2)$. Wählt man nun $f(F) = 2n^2$, so ist die Erfolgswahrscheinlichkeit von $\frac{1}{2}$ garantiert.
	\end{theorem}
	\section{Probabilistische Algorithmen}
	\subsection{Min-Cut}
	Dieses Problem ist in P. Eine effiziente Lösung ist durch den Ford-Fulkerson Algorithmus mithilfe dem Max-Flow/Min-Cut Lemmas möglich. Im Folgenden wird ein effizienter probabilistischer Algorithmus besprochen.
	\begin{enumerate}
		\item WHILE $\left|V\right| > 2$ DO
		\item wähle $(u,v) \in_R E$
		\item kontrahiere $(u,v)$
		\item ENDWHILE
	\end{enumerate}
	\begin{theorem}
		Der Algorithmus hat Laufzeit $\mathcal{O}(n^2 (\log n)^2)$. Der Algorithmus findet immer einen Schnitt. Der Algorithmus benötigt für jede Instanz $n-1$ Schritte. Für einen gegebenen minimalen Schnitt $C$ ist die Wahrscheinlichkeit, dass der Algorithmus $C$ findet ist $\geq \frac{2}{n(n-1)}$. Es genügen also $\mathcal{O}(n^2)$ Wiederholungen für eine konstant kleine Fehlerwahrscheinlichkeit.
	\end{theorem}
	In jedem Schritt des Algorithmus wird gehofft, dass keine Kante $e$ aus dem minimal cut $C$ gezogen wird. Im letzten Schritt beträgt die Fehlerwahrscheinlichkeit dafür aber $\frac{2}{3}$. Um dem entgegen zu wirken, ist die Idee, den Algorithmus ein wenig umzubauen. Nach etwa $n\left(\frac{\sqrt{2} - 1}{\sqrt{2}}\right)$ Schritten wird der bisher erzeugte Teilgraph $H$ von $G$ bestimmt. Für diesen Graphen wird der Algorithmus zwei separate Male ausgeführt und der kleinere der beiden Cuts wird gewählt. Eine Kontraktion ist in $\mathcal{O}(n)$ mithilfe der Adjazenzmatrix möglich, also wird die Laufzeit \[T(n) = \underbrace{\left(1-\frac{1}{\sqrt{2}}\right)n}_{\text{Anzahl Runden}} \cdot \underbrace{cn}_{\text{Laufzeit Kontraktion}} + \underbrace{2T\left(\frac{n}{\sqrt{2}}\right)}_{\text{Laufzeit Rekursion}}\]
	Mittels Master-Theorem lässt sich die Laufzeit bestimmen als $T(n) = \mathcal{O}(n^2 \log n)$.\\
	Die Wahrscheinlichkeit, dass $C$ nun die ersten $n\left(1-\frac{1}{\sqrt{2}}\right)$ Schritte überlebt, ist nun $\approx \frac{1}{2}$. Mittels induktivem Einsetzen kann überprüft werden, dass die Erfolgswahrscheinlichkeit $p(n) \geq \frac{1}{\log n}$ ist. Es genügen daher $\log n$ Wiederholungen für eine konstante Fehlerwahrscheinlichkeit. Die gesamte Laufzeit ist daher $\mathcal{O}(n^2 (\log n)^2)$.
	\subsection{All pairs shortest paths (APSP)}
	Wird jeder Pfad explizit ausgegeben, so kann die Ausgabe sehr groß sein. Man kann leicht ein Beispiel konstruieren, wo die gesamte Länge aller kürzesten Pfade $\Omega(n^3)$ ist. In diesem Fall wäre ein effizienter Algorithmus rein akademisch, da die Ausgabe ohnehin hohe Laufzeit benötigt.\\
	\begin{definition}
		Das Problem wird gelöst mithilfe der \underline{Shortest Path Matrix} $S \in \mathbb{R}^{n\times n}$. Diese ist definiert als $S_{i,j} = k \in V$, wenn $k$ Nachbar von $i$ in einem kürzesten $i-j$ Pfad ist. Bemerke, dass $S$ nicht eindeutig ist, da es mehrere kürzeste Wege geben kann. Mittels Dijkstra kann diese Matrix in $\mathcal{O}(n^3)$ berechnet werden. 
	\end{definition}
	Der folgende Algorithmus berechnet $S$ indem Matrizenmultiplikation verwendet wird. Sei $MM(n)$ die beste bekannte Laufzeit für Matrixmultiplikation, dann ist die Laufzeit von dem Algorithmus $\mathcal{O}(MM(n)\log^2 n)$. 
	\begin{definition}
		Wir definieren die Distanzmatrix $D$ mittels $d_{i,j}$ = Länge eines kürzesten $i-j$-Pfades. Wir schreiben $\delta(G)$ für den Durchmesser eines Graphen.
	\end{definition}
	\begin{lemma}
		Für die Adjazenzmatrix eines Graphen $G$, $u,v \in V$ und $k \in \mathbb{N}$ ist $A^k$ die Matrix aller $k$-Pfade. In anderen Worten $a^k_{u,v}$ ist die Anzahl der $u-v$-Pfade der Länge $k$.
	\end{lemma}
	Für $Z=A^2$ kann man einen Graphen $G'$ mit Kanten $E'$ definieren, sodass \[E' = E \cup \{(i,j) \mid d_{i,j} = 2\}\] Die Adjazenzmatrix ist dann $A'$ gegeben durch \[a'_{i,j} = \begin{cases}
		0, & i=j \lor (z_{i,j} = 0 \land a_{i,j} = 0)\\
		1, & \text{ sonst}
	\end{cases}\]
	Es folgt, dass $A'$ in $MM(n)$ berechnet werden kann.\\
	Es ist nun klar, dass falls $\delta(G) \leq 2$, dann ist $D=2A'-A$.
	\begin{lemma}
		Für alle $i,j \in V$ gilt $d_{i,j}$ gerade $\Rightarrow$ $d_{i,j} = 2d'_{i,j}$ und $d_{i,j}$ ungerade $\Rightarrow$ $d_{i,j} = 2d'_{i,j}-1$.
	\end{lemma}
	\begin{lemma}
		Für alle $i,j \in V$ und $\forall k \in N(i)$ ist $d_{i,j} - 1 \leq d_{k,j} \leq d_{i,j}+1$ und $\exists k \in N(i)$ sodass $d_{i,j}-1 = d_{k,j}$.
	\end{lemma}
	\begin{lemma}
		$\forall i,j \in V$ gilt $\forall k \in N(i)$, dass $d_{i,j}$ gerade $\Rightarrow$ $d'_{k,j} \geq d'_{i,j}$ und \\
		$d_{i,j}$ ungerade $\Rightarrow$ $d'_{k,j} \leq d'_{i,j}$ und $\exists k \in N(i)$ sodass $d'_{k,j} < d'_{i,j}$.\\
		Insbesondere folgt $d_{i,j}$ gerade $\Rightarrow \sum_{k \in N(i)} d'_{k,j} \geq d(i)\cdot d'_{i,j}$ und\\
		$d_{i,j}$ ungerade $\Rightarrow \sum_{k \in N(i)} d'_{k,j} < d(i)\cdot d'_{i,j}$.\\
		Weiter kann man sehen, dass $\sum_{k \in N(i)} d'_{k,j} = (A\cdot D')_{i,j}$.
	\end{lemma}
	Der Algorithmus zur Berechnung der $D$ Matrix (genannt $APD$, \underline{all pairs distance}) ist nun\\
	\underline{Funktion APD($A$ Adjazenzmatrix von $G$):}
	\begin{enumerate}
		\item $Z = A^2$
		\item Berechne $A'$
		\item if $\forall i,j \in V, \; i\neq j$ gilt $a'_{i,j} = 1$ then return $D = 2A'-A$
		\item $D' = APD(A')$
		\item $F = A\cdot D'$
		\item for each $(i,j) \in V\times V$:
		\item if $f_{i,j} \geq d'_{i,j}-1$ then $d_{i,j} = 2d'_{i,j}$
		\item else $d_{i,j} = 2d'_{i,j}-1$
		\item return $D$
	\end{enumerate}
	Bezeichne $T(n,k)$ die Laufzeit des Algorithmus für einen Graphen auf $n$ Knoten mit Durchmesser $k$. Wir wissen, dass $T(n,2) = MM(n)$. Außerdem ist \[T(n,k) = MM(n) + \mathcal{O}(n^2) + T\left(n,\left\lceil\frac{k}{2}\right\rceil\right) + MM(n) = \mathcal{O}(MM(n)\cdot \log(n)\]
	\subsection{Boolean Product Witness}
	Seien $A,B \in \{0,1\}^{n\times m}$.
	\begin{definition}
		Wir definieren $A\odot B = C$ durch \[c_{i,j} = \lor_{k=1}^n a_{i,k} \land b_{k,j}\]
		Die BPW Matrix ist definiert durch \[w_{i,j} = \begin{cases}
			0, & c_{i,j} = 0\\
			k, & c_{i,j} = 1 \text{ und } a_{i,k} \land b_{k,j} = 1
		\end{cases}\]
	Im zweiten Fall heißt $w_{i,j} = k$ ein Zeuge.
	Außerdem definiert man die Matrix $\hat{A}$ durch $\hat{a}_{i,k} = k a_{i,k}$. Diese kann in $\mathcal{O}(n^2)$ berechnet werden.
	\end{definition}
	Gibt es für $(i,j)$ genau einen Zeugen, so gilt \[(\hat{A}\cdot B)_{i,j} = k\]
	Sei nun $w>0$ die Anzahl der Zeugen für eine Position $(i,j)$. Sei außerdem $r = 2^s$ für ein $s \in \mathbb{N}$, sodass $$\frac{n}{2} \leq r\cdot w \leq n$$
	Unter Gleichverteilung werden nun zufällig $r$ Elemente aus $\{1,...,n\}$ gezogen und daraus die Menge $R$ erstellt. Es gilt dann \[\mathbb{P}[\text{in $R$ gibt es genau einen Zeugen für $(i,j)$}] = p = \frac{w\binom{n-w}{r-1}}{\binom{n}{r}}\]
	\begin{lemma}
		$p \geq \frac{1}{2^e}$
	\end{lemma}
	\begin{definition}
		Wir definieren $B^R$ als die Matrix, die aus $B$ gewonnen wird, wenn alle Zeilen $z$ mit $z \notin R$ auf 0 gesetzt werden.
	\end{definition}
	Falls es in $R$ genau einen Zeugen $l$ für $A,B$ gibt, so gilt $(\hat{A}B^R)_{i,j} = l$. 
	Auf dieser Basis kann nun ein Algorithmus zur Lösung des BPW Problems konstruiert werden.
	\begin{enumerate}
		\item $W = -A\cdot B$
		\item for $t=0$ to $\lfloor\log n\rfloor$
		\item $r = 2^t$
		\item for $l = 1$ to $\lceil2e \log n\rceil$
		\item wähle $R \in_R \{1,...,n\}$ mit $\left|R\right| = r$
		\item Berechne $\hat{A}$, $B^R$ und $Z = \hat{A} B^R$.
		\item for $(i,j)$
		\item if $w_{i,j}<0$ and $z_{i,j}$ ist Zeuge then $w_{i,j} = z_{i,j}$
		\item endfor
		\item endfor
		\item endfor
		\item for $(i,j)$ finde Zeuge $w_{i,j}$
	\end{enumerate}
	Die Laufzeit des Algorithmus ist beschränkt auf $\mathcal{O}(MM(n) \cdot \log^2 n)$.
	Mit dieser Überlegung kann nun das APSP Problem gelöst werden. Dafür ist anzumerken, dass für $j \in N_G(i)$ gilt, dass die Distanz zu $k$ $d_{i,k} = d_{j,k} + s$ für $s \in \{-1,0,1\}$ erfüllt.\\
	\begin{enumerate}
		\item $D = APD(A)$
		\item for $s \in \{0,1,2\}$
		\item Berechne $D^{(s)}$ via $d_{i,j}^{(s)} = 1 \Leftrightarrow d_{i,j} = s-1 \mod 3$
		\item $W^{(s)} = BPW(A,D^{(s)})$
		\item endfor
		\item Berechne $S$ via $s_{i,j} = w_{i,j}^{d_{i,j} \mod 3}$
	\end{enumerate}
	Die Gesamtkomplexität des Algorithmus ist $\mathcal{O}(MM(n) \log^2 n)$.
	\subsection{Perfektes Matching auf bipartiten Graphen}
	Das Problem ist effizient deterministisch zu lösen, aber Algorithmen sind üblicherweise sehr kompliziert. Wir stellen daher stattdessen einen einfachen probabilistischen Algorithmus vor.
	\begin{definition}
		Sie $G = (V_1 \cup V_2, E)$ ein biparttier Graph mit $\left|V_1\right| = \left|V_2\right| = n$. Man definiert die $n\times n$ Bipartitionsmatrx $A_G$ durch \[A_G = \begin{cases}
			x_{i,j}, & (i,j) \in E\\
			0, & \text{ sonst}
		\end{cases}\]
	Die Determinante $D$ der Matrix wird wie üblich bestimmt als \[\det(A) = \sum_{\pi \in S_n} sgn(\pi) \prod_{i=1}^{n} a_{i,\pi(i)}\]
	Es ist offensichtlich, dass $\det(A_G)$ ein Polynom in den $x_{i,j}$ ist.
	\end{definition}
	\begin{theorem}
		Es gibt ein perfektes Matching in $G$ $\Leftrightarrow$ $\det(A_G) \not \equiv 0$.
	\end{theorem}
	Zu bestimmen, ob die Determinante der Matrix das Null-Polynom ist, kann im Allgemeinen sehr aufwändig sein. Ein probabilistischer Ansatz ist es, zufällige Werte in das Polynom einzusetzen und zu überprüfen, ob die Funktion 0 ausgibt. Dabei hilft das folgende Lemma.
	\begin{lemma}
		Sei $g$ ein Polynom mit $m$ Variablen und Grad $\leq d$. Dann hat $p$ $\leq mdM^{m-1}$, wenn die Parameter $a_1,...,a_m \in \{0,1,...,M-1\}$.
	\end{lemma}
	Für das Polynom der Determinanten gilt $m\leq n^2, d=1$. Der Algorithmus zum Überprüfen, ob das Polynom dem Nullpolynom entspricht, besteht nun daraus, zufällige Werte für die $x_{i,j}$ zu wählen und die Determinante der daraus entstandenen Matrix zu berechnen.
	\begin{enumerate}
		\item Berechne $A_G$
		\item belege $x_{i,j} \in_R \{0,...,M-1\}$ für alle $i,j$
		\item setze diese Werte in $A_G$ ein und bezeichne das Resultat als $A_G'$
		\item if $\det(A_G') \neq 0$: es gibt perfektes Matching
		\item else: es gibt (wahrscheinlich) kein perfektes Matching
	\end{enumerate}
	Der Algorithmus hat eine Fehlerwahrscheinlichkeit $\leq \frac{n^2}{M}$. Wählt man $M = 2n^2$, so ist die Fehlerwahrscheinlichkeit $\leq \frac{1}{2}$.
	\subsection{Perfektes Matching auf allgemeinen Graphen}
	Aus offensichtlichen Gründen gehen wir davon aus, dass $n$ gerade ist.
	\begin{definition}
		Wir definieren wieder eine leicht veränderte Adjazenzmatrix
		\[(A_G)_{i,j} = \begin{cases}
			0, & (i,j) \notin E\\
			x_{i,j}, & (i,j) \in E \land i<j\\
			-x_{i,j}, & (i,j) \in E \land j<i
		\end{cases}\]
	\end{definition}
	Ein Matching ist dann eine Permutation $\pi$ mit $\pi^2 = id$ und $\pi(i) \neq i$. Die Idee ist nun, wieder über die Determinante der Matrix zu argumentieren. Es gilt \[\det(A_G) = \sum_{\pi \in S_n} sgn(\pi) \prod_{i=1}^{n} a_{i,\pi(i)}\] Es gilt wieder das selbe Resultat wie für bipartite Graphen. \begin{lemma}
		$G$ hat ein perfektes Matching $\Leftrightarrow \det(A_G) \not\equiv 0$.
	\end{lemma}
	Es ist klar, dass der Algorithmus für bipartite Graphen dann auch für allgemeine Graphen funktioniert.
	\subsection{Perfektes Matching finden}
	Mit den vorherigen Teilen kann nun bestimmt werden, ob in einem Graphen ein Matching existiert. Ein solches zu finden, benötigt aber zunächst einige Überlegungen.
	\begin{definition}
		Sei $S = \{x_1,...,x_n\}$ eine endliche Menge. $F = \{S_1,...,S_k\}$ sodass $S_j \subseteq S$ heißt Mengensystem. Es ist klar, dass $\left|F\right| \leq 2^n$\\
		Sei $w:S \to \{1,2,...,2n\}$ eine Funktion die jedem Element aus $S$ ein Gewicht zuordnet. $w(S_j)$ sei eine verkürzte Schreibweise für $\sum_{x \in S_j} w(x)$.
	\end{definition}
	\begin{lemma}[Isolierungslemma]
		Ist $w$ zufällig und unter Gleichverteilung gewählt, so ist \[\mathbb{P}[\exists ! S_i \in F : w(S_i) \text{ minimal }] \geq \frac{1}{2}\]
	\end{lemma}
	Die Idee ist nun, mit diesem Lemma einen Algorithmus zur Bestimmung eines perfekten Matchings in bipartiten Graphen zu konstruieren. Auf Existenz eines perfekten Matchings können wir bereits überprüfen.\\
	Sei nun $G = (U\cup V,E)$ ein bipartiter Graph mit perfektem Matching und $\left|U\right| = \left|V\right|$. Wir statten die Kanten mit Gewichten $w$ in $\{1,2,...,2m\}$ aus. Im Rahmen des Lemmas sein $S = E$ und $F$ die Menge der perfekten Matchings. Wir definieren $A \in \mathbb{R}^{n\times n}$ mit \[a_{i,j} = \begin{cases}
		1, & (u_i,v_j) \in E\\
		0, & \text{ sonst}
	\end{cases}\]
	Daraus definieren wir eine zweite Matrix $B$ mit \[b_{i,j} = \begin{cases}
		2^{w_{i,j}}, & (u_i,v_j) \in E\\
		0, & \text{ sonst}
	\end{cases}\]
	\begin{lemma}
		Falls $G$ genau ein perfektes Matching mit minimalem Gewicht $w$ hat, so ist $\det(B) \neq 0$ und $2^w$ ist die größte Zweierpotenz die $\det(B)$ teilt.
	\end{lemma}
	\begin{lemma}
		Gibt es in $G$ genau ein perfektes Matching $M$ minimalen Gewichts $w$, dann gilt \[(u_i,v_j) \in M \Leftrightarrow \frac{\det(B_{i,j})}{2^w} \mod 2 = 0\] Dabei ist $B_{i,j}$ die Matrix $B$ mit 0 an der Stelle $(i,j)$.
	\end{lemma}
	Der Algorithmus ist nun klar. Man überprüft einfach für jede Kante, ob das obige Lemma erfüllt ist. Wenn ja, wird sie in das Matching aufgenommen.
	\subsection{Rot-Blau-perfektes Matching}
	Das folgende Problem kann mit einem probabilistischen Algorithmus effizient gelöst werden. Ein polynomieller deterministischer Algorithmus ist hingegen nicht bekannt.\\
	Gegeben sei ein bipartiter Graph $G=(U\cup V, E)$ mit $\left|U\right| = \left|V\right| = n$ und mit einer Färbung $c$ der Kanten $c:E \to \{r,b\}$, jede Kante ist also rot oder blau. Außerdem gibt es ein $k\leq n$. Die Frage ist nun, gibt es ein perfektes Matching mit genau $k$ roten Kanten?\\
	Die Idee ist wieder, das Isolationslemma zu verwenden. Dafür wählen wir $S=E$ und $F$ als die Menge der perfekten Matchings mit genau $k$ roten Kanten. Weiter definieren wir uns $f:E \to \{p,2p\}$ durch \[f(e) = \begin{cases}
		2p, & c(e) = r\\
		p, & c(e) = b
	\end{cases}\] wobei $p=3n^3$.
	Daraus wird nun wieder eine Matrix gebaut, $B \in \mathbb{R}^{n\times n}$ mit \[b_{i,j} = \begin{cases}
		2^{w_{i,j} + f((i,j))}, & (i,j) \in E\\
		0, & \text{ sonst}
	\end{cases}\]
	Für ein perfektes Matching $\sigma$ mit genau $k$ roten Kanten gilt nun $$\prod_{i=1}^n b_{i,\sigma(i)} = 2^{w(\sigma) + (n+k)p} \leq 2^{n\cdot 2m + (n+k)p} \leq 2^{2n^3 + (n+k)p} \leq 2^{(n+k+1)p}$$ Es folgt, dass die Terme in $\det(B)$, die ein perfektes Matching mit mehr als $k$ roten Kanten implizieren, diesen Term nicht annullieren können. Es gibt außerdem höchstens $n!$ viele perfekte Matchings mit weniger als $k$ roten Kanten. Die Summe solcher Terme ist kleiner als $2^{n^2+(n+k-1)p+2n^3}$ was wiederum kleiner als $2^{w(\sigma)+(n+k)p}$ ist. Diese Summe kann den Term also auch nicht annullieren.\\
	Der Algorithmus wählt die Gewichtsfunktion $w$ zufällig und berechnet $\det(B)$. Das ist eine Summe von Zweierpotenzen. Der Algorithmus gibt aus, dass es ein perfektes Matching mit $k$ roten Kanten gibt, genau dann, wenn in dieser Summe eine Zweierpotenz $2^r$ vorkommt, sodass $2^{(k+1)p} <2^r <2^{(k+n)p}$.\\
	Interessant an diesem Problem ist, dass es ein NP Problem ist. Das heißt, man kann es probabilistisch zwar effizient lösen, ein deterministischer, polynomieller Algorithmus ist aber nicht bekannt.
	\subsection{Probabilistische Algorithmen in exponentieller Zeit}
	\subsubsection{3-Färbbarkeit}
	Das Problem ist NP-vollständig. Wir werden zeigen, dass es einen probabilistischen Algorithmus mit Laufzeit $\mathcal{O}(p(n)c^n) := \mathcal{O}^*(c^n)$ gibt.\\
	Die Idee des Algorithmus ist es, das Graphenproblem in ein SAT Problem umzuschreiben. Für jeden Knoten $v \in V$ und jede Farbe $j \in \{1,2,3\}$ wählen wir eine boole'sch Variable $x_{v,j}$, die 1 ist, genau dann, wenn $v$ Farbe $j$ bekommt. In der Formel $F_G$ gibt es nun zwei Arten von Klauseln.
	\begin{enumerate}
		\item $\forall v \in V$: $(x_{v,1} \lor x_{v,2} \lor x_{v,3})$
		\item $\forall (u,v) \in E$: $(\overline{x_{v,1}} \lor \overline{x_{u,1}}) \land (\overline{x_{v_2}} \lor \overline{x_{u,2}}) \land (\overline{x_{v,3}} \lor \overline{x_{u,3}})$
	\end{enumerate}
	Eine erfüllende Belegung der Formel impliziert nun eine Färbung des Graphen.\\
	Der Algorithmus wählt für jeden Knoten $v$ eine Farbe $j$, die für $v$ ausgeschlossen wird. D.h. $v$ wird nicht mir $j$ gefärbt und $x_{v,j}=0$. Die Klauseln von Typ 1 werden dadurch reduziert auf zwei Literale und dadurch ist $F_G$ reduziert auf eine Formel in der Familie 2-SAT. Diese lässt sich effizient lösen. Wir berechnen nun die Wahrscheinlichkeit, dass die reduzierte Formel $F_G$ lösbar ist, wenn gegeben ist, dass $G$ 3-färbbar ist. Sei $f$ eine korrekte 3-Färbung für $G$. Die Wahrscheinlichkeit, dass der Algorithmus für einen beliebigen Knoten $v$ genau die Farbe $f(v)$ ausschließt, ist $\frac{1}{3}$. Die Wahrscheinlichkeit, dass kein Knoten seine Farbe verliert, ist also $\left(\frac{2}{3}\right)^n$. Um die Wahrscheinlichkeit für einen Fehler gering zu halten, wird der Algorithmus einfach $l$ mal wiederholt. Bei $l=100\left(\frac{3}{2}\right)^n$ ist der die Fehlerwahrscheinlichkeit $\leq 2^{-100}$. Die Laufzeit ist demnach $\mathcal{O}(p(n)\left(\frac{3}{2}\right)^n) = O^*(\left(\frac{3}{2}\right)^n)$.
	\subsubsection{Random Walk Algorithmus für 3-SAT}
	Der folgende Algorithmus versucht, 3-SAT zu lösen.
	\begin{enumerate}
		\item beginne mit einer zufälligen Belegung $\alpha$
		\item if $\alpha(F)\neq 1$, dann wähle eine Klausel $C$, sodass $\alpha(C) = 0$
		\item wähle ein Literal $u_l$ in $C$ und verändere $\alpha$, sodass $u_l$ erfüllt (und damit $C$ erfüllt)
		\item wiederhole $n$ mal
		\item wenn $\alpha$ nicht zu einer erfüllenden Belegung umgewandelt werden konnte, wähle eine neue
		\item wiederhole $t$ mal
	\end{enumerate}
	Sei $\alpha_0$ eine erfüllende Belegung, die Hamming-Distanz $p$ zur initialen Belegung $\alpha$ hat. Dann transformiert der random walk $\alpha$ in $\alpha_0$ mit Wahrscheinlichkeit $3^{-p}$. Wird die Laufzeit nun aber verlängert auf (zum Beispiel) $3p$ Schritte, dann dürfen $p$ Schritte des random walks falsch sein, wenn dafür $2p$ richtig sind. Die Wahrscheinlichkeit, dass $\alpha_0$ gefunden wird, ist damit $\binom{3p}{2p}\left(\frac{1}{3}\right)^{2p}\left(\frac{2}{3}\right)^p \approx 2^{-p}$. Man kann zeigen, dass die Annahme $p=\frac{n}{3}$ optimal ist. Deswegen wird im Algorithmus der random walk auf eine Länge von $n=3p$ beschränkt.\\
	Zur weiteren Untersuchung definieren wir
	\begin{itemize}
		\item $E_1$ ist das Zufallsereignis, dass $d(\alpha_0,\alpha) = \frac{n}{3}$
		\item $E_2$ ist das Zufallsereignis, dass der Algorithmus $\frac{2n}{3}$ Schritte in die richtige und $\frac{n}{3}$ Schritte in die falsche Richtung macht.
	\end{itemize}
	Es gilt dann, dass die Erfolgswahrscheinlichkeit $\geq \mathbb{P}[E_1\land E_2]$ ist. Es gilt \[\mathbb{P}[E_1] = \binom{n}{\frac{n}{3}}\cdot 2^{-n}\] und \[\mathbb{P}[E_2] = \binom{n}{\frac{n}{3}}\left(\frac{1}{3}\right)^{\frac{2n}{3}}\left(\frac{2}{3}\right)^\frac{n}{3}\]
	Damit sieht man, dass die Erfolgswahrscheinlichkeit $\geq \frac{1}{q(n)}\left(\frac{3}{4}\right)^n$ ist. Wählt man $t = 20\cdot \left(\frac{4}{3}\right)^n$, so ergibt sich insgesamt eine Fehlerwahrscheinlichkeit von $\leq e^{-20}$. Die Laufzeit ist $\mathcal{O}^*(\left(\frac{4}{3}\right))^n$.
	\section{Approximationsalgorithmen}
	Sei $P$ ein Problem und $X$ eine Menge von Instanzen. Für $x \in X$ nennen wir $S_x$ die Menge der Lösungen für $x$. Auf der Menge $S = \bigcup_{x \in X} S_x$ definieren wir die Kostenfunktion $m$. Gesucht ist $opt(x)$ der minimale (bzw. maximale) Wert einer Lösung für alle Instanzen $x$.
	\begin{definition}
		$A$ ist ein $\varepsilon$-Approximationsalgorithmus, wenn \[m(A(x)) \geq (1-\varepsilon) opt(x)\] für Maximierungsprobleme und \[m(A(x)) \leq \frac{1}{1-\varepsilon} opt(x)\]
	\end{definition}
	\begin{definition}[(F)PTAS]
		Ein Approximationsalgorithmus $A$ für ein Problem $P$ ist ein PTAS, wenn $A$ mit Eingaben $x$ und $\varepsilon$ eine $\varepsilon$-approximative Lösung für $x$ produziert und die Laufzeit polynomiell in $\left|x\right|$ ist. Hängt die Laufzeit zusätzlich von $\frac{1}{\varepsilon}$ ab, so ist $A$ ein FPTAS.
	\end{definition}
	\subsection{Max-Cut}
	Gegeben $G=(V,E)$. Finde $C\subset V$, sodass \[m(C) = \left|\{(u,v) \mid u \in C \; v \notin C\}\right|\] maximal. Das Entscheidungsproblem ist NP-vollständig.
	\begin{definition}
		Sei $C\subseteq V$ und $v \in V$. Dann definieren wir \[C\Delta \{v\} = \begin{cases}
			C \setminus \{v\}, & v \in C\\
			C \cup \{v\}, & v \notin C
		\end{cases}\]
	\end{definition}
	Damit können wir einen Algorithmus bauen.
	\begin{enumerate}
		\item $C = \varnothing$
		\item while $\exists v \in V$ mit $m(C\Delta v) > m(C)$
		\item $C = C\delta \{v\}$
		\item end
	\end{enumerate}
	\begin{lemma}
		Die Laufzeit ist $\mathcal{O}(n^4)$. Die gefundene Lösung $C$ erfüllt $\left|C\right| \leq \frac{1}{2}opt$. 
	\end{lemma}
	\subsection{Max-$k$-SAT}
	Sei $F = C_1 \land \dots \land C_m$ eine Formel in der jede Klausel genau $k$ Literale hat. Finde eine Belegung $\alpha: Var(F) \to \{0,1\}$, die \[m(\alpha) = \left|\{C_i \mid \alpha(C_i) = 1\}\right|\] maximiert. Wir geben einen Algorithmus mit Approximationsfaktor $\varepsilon = \frac{1}{2^k}$ an.\\
	Sei dafür $a \in_R \{0,1\}^n$ eine zufällige Belegung. Wir definieren $f(F)$ als die erwartete Anzahl an Klauseln die eine zufällige Belegung erfüllt. Dafür definieren wir die Zufallsvariable $$X_i = \begin{cases}
		1, & \alpha \text{ erfüllt } C_i\\
		0, & \text{ sonst}
	\end{cases}$$ Offensichtlich ist $\mathbb{P}[X_i] = \mathbb{P}[\alpha(C_i) = 1] = 1 - \frac{1}{2^{\left|C_i\right|}} = 1-\frac{1}{2^k}$. Wir schließen \[f(F) = m\left(1-\frac{1}{2^k}\right)\]
	Dieser Wert lässt sich aber auch anders bestimmen. Sei $\#(F,\alpha)$ die Anzahl der Klauseln in $F$m die von $\alpha$ erfüllt werden. Sei $\overline{a} = (a_2,a_3,...,a_n)$. \[f(F) = \sum_{\alpha \in \{0,1\}^n} \mathbb{P}[a = \alpha] \cdot \#(F,\alpha) = \sum_{\alpha \in \{0,1\}^{n-1}} \mathbb{P}[a = 0\alpha] \cdot \#(F,0\alpha) + \sum_{\alpha \in \{0,1\}^{n-1}} \mathbb{P}[a = 1\alpha] \cdot \#(F,1\alpha)\]
	\[= \frac{1}{2}\left(\sum_{\alpha \in \{0,1\}^{n-1}} \mathbb{P}[\overline{a} = \alpha] \cdot \#(F,0\alpha) + \sum_{\alpha \in \{0,1\}^{n-1}} \mathbb{P}[\overline{a} = \alpha] \cdot \#(F,1\alpha)\right)\]
	\[= \frac{1}{2} \sum_{\alpha \in \{0,1\}^{n-1}} \mathbb{P}[\overline{a} = \alpha] \cdot \left(\#(F,0\alpha) + \#(F,1\alpha)\right) \leq \sum_{\alpha \in \{0,1\}^{n-1}} \mathbb{P}[\overline{a} = \alpha] \cdot \#(F|_{x_1=0},\alpha) = f(F|_{x_1=0})\]
	Das motiviert den Algorithmus
	\begin{enumerate}
		\item for $i=1$ to $n$
		\item $p_0 = f(F|_{x_i=0})$, $p_1 = f(F|_{x_i=1})$
		\item if $p_0 \geq p_1$: $F=F|_{x_i=0}$
		\item else: $F=F|_{x_i=1}$
		\item endif
		\item endfor
	\end{enumerate}
	\subsection{0-1 Rucksackproblem}
	Wir betrachten eine Menge an $n$ Objekten $i$, die jeweils mit einem Gewicht $g_i$ und einem Wert $v_i$ ausgestattet sind. Wir bezeichnen mit $G$ die Größe des Rucksacks. Für eine bestimmte Instanz $x$ des Problems bezeichne $S_x$ die Menge aller Lösungen. Wir beginnen mit einem Approximationsalgorithmus für $\varepsilon = \frac{1}{2}$.\\
	Der \underline{Greedy-Algorithmus} sortiert die Elemente nach ihrem Kosten-Nutzen-Verhältnis \[\frac{v_1}{g_1} \geq \frac{v_2}{g_2} \geq \dots \geq \frac{v_n}{g_n}\] Ist $i$ das erste Objekt, das nicht mehr zum Rucksack ergänzt werden kann, wenn die ersten $i-1$ Elemente gewählt wurden, dann wähle die bessere der beiden Lösungen $S_1=\{1,2,\dots,i-1\}, S_2 = \{i\}$. Es ist nicht schwer zu sehen, dass der Algorithmus eine $\frac{1}{2}$-Faktor Approximation erreicht.\\
	Für das Problem existiert außerdem ein FPTAS. Dieser basiert auf einem Ansatz der dynamischen Programmierung. Hierbei kann die Zielfunktion $w(i,h)$ rekursiv wie folgt bestimmt werden \[w(i,h) = \begin{cases}
		0, & h=0\\
		w(i-1,h), & g_i > h\\
		\max\{w(i-1,h), w(i-1, h-g_i)+v_i\} & g_i \leq h
	\end{cases}\]
	Auf diesem Ansatz aufbauend, kann man sich eine Art duale Formulierung des Problems vorstellen. Dabei wird die Funktion \[g(i,h) = \min_{S\subseteq \{1,2,...,i\}} \{\sum_{i \in S} g_i \mid \sum_{i \in S} v_i \geq h\}\]
	die sich wiederum rekursiv ermitteln lässt \[g(i,h) = \begin{cases}
		0, & h=0\\
		\infty, & \sum_{j=1}^{i} v_i < h\\
		\min\{g(i-1,h), g(i-1, h-v_i) - g_i\}, & \text{ sonst}
	\end{cases}\]
	Die Tabellengröße ist dabei $n^2v_{\max}$.\\
	Mit diesem Ansatz lässt sich nun ein FPTAS formulieren. Dafür werden zunächst die Werte $v_i$ binär gerundet. Das heißt, wir definieren \[v_i' = 2^b \lceil\frac{v_i}{2^b}\rceil\] Hierbei ist $b$ eine Konstante. Die Transformation wirkt sich auf die Binärdarstellung von $v_i$ so aus, dass die letzten $b$ Stellen auf 0 gesetzt werden. Führt man nun den obigen Algorithmus auf die Werte $(\frac{v_1'}{2^b},\frac{v_2'}{2^b},...,\frac{v_n'}{2^b})$ aus, so ist die Laufzeit nun durch $n^2\frac{v_{\max}}{2^b}$ beschränkt. Wir bezeichnen \[m(A(x)) = \sum_{i \in S'} v_i\] die vom Algorithmus gefundene Lösung und \[opt(x) = \sum_{i \in S} v_i\] die optimale Lösung. Es gilt dann \[F = \frac{opt(x) - m(A(x))}{m(A(x))} \leq \frac{n\cdot 2^b}{v_{\max}} \leq \varepsilon\] Für den FPTAS wird nun $b$ so gewählt, dass \[2^b \leq \frac{\varepsilon v_{\max}}{n} < 2^{b+1}\]
\subsection{Approximierbarkeit}
	\subsubsection{Travelling Salesman}
	Gegeben sei die Distanzmatrix $D$ eines vollständigen Graphen $G$. Gesucht ist ein kürzester Hamilton Pfad. Wir haben bereits gesehen, dass dieser als eine Permutation $\pi$ der Knoten geschrieben werden kann.
	\begin{theorem}
		Falls $P\neq NP$, so existiert kein $\varepsilon$-Approximationsalgorithmus für TSP in polynomieller Zeit.
	\end{theorem}
	\subsubsection{Clique}
	Das Cliquen-Problem in einem Graphen $G$ besteht darin, eine maximale Clique $C$ zu finden. Auch dieses Problem lässt sich nicht approximieren.
	\begin{theorem}
		Falls es ein $\varepsilon$ mit einem $\varepsilon$-Approximationsalgorithmus für das Cliquen Problem gibt, so gibt es einen solchen Algorithmus für alle $\varepsilon \in (0,1)$.
	\end{theorem}
\subsection{$\Delta$-TSP}
	Das $\Delta$-TSP oder metrisches TSP Problem bezeichnet die Suche nach einem minimalen Hamilton-Pfads in einem Graphen $G$, der die Dreiecksungleichung erfüllt.
	\begin{definition}
		Eine quadratische Matrix $D$ heißt Metrik, wenn alle ihre Einträge $\geq 0$ sind und $\forall i,j,k$ gilt \[d_{i,k} \leq d_{i,j} + d_{j,k}\]
	\end{definition}
	\begin{lemma}
		HK $\leq \Delta$-TSP
	\end{lemma}
	Wir beschreiben nun zwei Approximationsalgorithmen für das metrische TSP.\\
	Gegeben sei die Distanzmatrix $D$.
	\begin{enumerate}
		\item Finde einen minimalen Spannbaum $T$
		\item Verdopple jede Kante in $T$, bezeichne diesen Graphen als $T_E$
		\item Bestimme einen Euler-Kreis $C$ in $T_E$
		\item Generiere $H$ aus $C$, indem alle Knoten $c_i$ mit $\exists j<i$ sodass $c_j=c_i$ gelöscht werden.
	\end{enumerate}
	\begin{theorem}
		Es ist nicht schwer zu sehen, dass dieser Algorithmus einen Approximationsfaktor von $\frac{1}{2}$ erreicht.
	\end{theorem}
	Der folgende Algorithmus nach Christofides erreicht sogar eine $\frac{1}{3}$-Faktor Approximation.
	\begin{enumerate}
		\item Finde einen minimalen Spannbaum $T$ in $G$
		\item Sei $V'$ die Menge der Knoten mit ungeradem Grad in $G$
		\item Finde kleinstes perfektes Matching $M$ in $V'$
		\item Sei $G_E = T\cup M$
		\item Finde Euler-Kreis in $G_E$
		\item Bestimme Hamilton-Kreis wie zuvor
	\end{enumerate}
	\begin{theorem}
		Der Algorithmus erreiche eine $\frac{1}{3}$-Faktor Approximation an $\Delta$-TSP.
	\end{theorem}
	\subsection{PCP Theorem}
	PCP steht für \underline{Probabilistically Checkable Proofs}. Dieses Resultat wird verwendet, um zu zeigen, dass manche Probleme nur bis zu einem bestimmten Faktor approximiert werden können. Die Klasse NP kann charakterisiert werden, als die Menge der Probleme, die polynomiell beweisbar sind.
	\begin{definition}
		Seien $r,q: \mathbb{N} \to \mathbb{N}$. Ein $(r,q)$-Beweis-Verfizierer für $L\subseteq \Sigma^*$ ist ein Polynomialzeit-Algorithmus $A$, der für Eingabe $x$ und einen Beweis $y$ (für $x$) $r(\left|x\right|)$ Zugallsbits generiert. $A(x,r)$ generiert eine Menge an Stellen $i_1,...,i_{p(\left|x\right|)} \in \mathbb{N}$. $A(x,r,y(i_1),... y(i_p)) \in \{0,1\}$ bestimmt of $x \in L$ oder $x \notin L$.
	\end{definition}
	\begin{definition}
		Eine Sprache $L$ ist in der Komplexitätsklasse $PCP(r,q)$, wenn es einen $(r,q)$ Beweis-Verfizierer gibt mit den Eigenschaften \begin{itemize}
			\item $x \in L \Rightarrow$ $\exists$ Beweis $B$, sodass $\mathbb{P}[A(x,r,B) = 1] = 1$
			\item $x \notin L \Rightarrow$ $\forall$ Beweise $B$ gilt $\mathbb{P}[A(x,r,B) = 1] \leq \frac{1}{2}$.
		\end{itemize}
	\end{definition}
	\begin{theorem}
		\begin{itemize}
			\item $NP = PCP(\mathcal{O}(\log n), \mathcal{O}(1))$
			\item $NP = PCP(\mathcal{O}(\log n), 3)$
			\item $P = PCP(\mathcal{O}(\log n), 2)$
		\end{itemize}
	\end{theorem}
	\begin{theorem}
		Falls $P\neq NP$, so existiert kein $\varepsilon$-Approximationsalgorithmus für Max-$k$-SAT mit $\varepsilon < \frac{1}{16}$.
	\end{theorem}
	\subsection{\#DNF-SAT}
	Im folgenden betrachten wir Formeln nicht mehr wie bisher in KNF sondern in DNF. Überprüfen auf Erfüllbarkeit einer DNF ist einfach, da eine Formel $F$ in DNF nicht erfüllbar ist, genau dann, wenn jede Klausel eine Variable und ihr Komplement enthält. Das Ziel des \#DNF-SAT Problems ist es, zu ermitteln, wie viele erfüllende Belegungen existieren. 
	\begin{definition}
		Sei $\mathcal{D}$ die Menge aller Formeln, die in DNF vorliegen. Wir definieren \[f: \mathcal{D} \to \mathbb{N}\] \[F \mapsto \left|\{\alpha: V(F) \to \{0,1\}^n \mid \alpha(F) = 1\}\right|\] die Funktion, die einer Formel $F$ die Anzahl der erfüllenden Belegungen zuordnet. 
	\end{definition}
	Ziel dieser Sektion ist es, einen probabilistischen Algorithmus $A$ zu konstruieren, der \[\mathbb{P}[(1-\varepsilon)f(F) \leq A(F) \leq (1+\varepsilon)A(F)] \geq 1-\delta\] erfüllt und eine Laufzeit hat, die polynomial in $\left|F\right|, \frac{1}{\varepsilon}$ und $\log(\frac{1}{\delta})$ ist.
	\begin{remark}
		Ist $F$ in KNF gegeben, so kann man $f(F)$ ermitteln mittels $f(F) = 2^n - f(\lnot F)$. Es gilt $f \in SAT \Leftrightarrow f(\lnot F) < 2^n$.
	\end{remark}
	\begin{remark}
		Dieses Problem ist ein Spezialfall eines allgemeineren Problems. Sei $U$ eine beliebige Menge und $\left|U\right|$ bekannt. Angenommen, es ist möglich schnell Elemente von $U$ unter Gleichverteilung zu wählen und angenommen, es gibt eine Funktion $f:U \to \{0,1\}$. Wir wollen für \[G = \{u \in U \mid f(u) = 1\}\] die Kardinalität $\left|G\right|$ ermitteln.
	\end{remark}
	Der folgende probabilistische Ansatz liefert eine erste Möglichkeit
	\begin{enumerate}
		\item $k=0$
		\item for $i=1$ to $N$
		\item wähle $u_i \in_R U$
		\item if $f(u_i) = 1$ then $k=k+1$
		\item output $\frac{\left|U\right|}{N}k$
	\end{enumerate}
	\begin{theorem}\label{thm: dnf sat ineq}
		Für $0<\varepsilon, \delta < 1$ und $N \geq \frac{4\left|U\right|}{\varepsilon^2 \left|G\right|} \ln \frac{2}{\delta}$ gilt die Abschätzung \[\mathbb{P}[(1-\varepsilon)\left|G\right| \leq A(U) \leq (1+\varepsilon) \left|G\right|] \geq 1-\delta\]
	\end{theorem}
	\begin{remark}
		Initial ist $\left|G\right|$ nicht bekannt, also kann man $N$ nicht passend wählen. Außerdem ist es möglich, dass $N$ nicht polynomial in $\left|F\right|$ wächst.
	\end{remark}
	Wir verfolgen einen besseren Ansatz. Sei dafür eine Formel in DNF \[F = I_! \lor I_2 \lor \dots \lor I_m\] mit den sogenannten Implikanten $I_i$. Definiere \begin{itemize}
		\item $U = \{(\nu.i) \mid \nu(I_i) = 1\}$ wobei $\nu$ erfüllende Belegungen für einen Implikanten sind
		\item $H_i = \{\nu \mid \nu(I_i) = 1\}$ und 
		\item $G = \{\nu \mid \nu(F) = 1\}$
	\end{itemize}
	Es gilt $\left|G\right| \leq \left|U\right| \leq \sum_{i=1}^{m} \left|H_i\right|$.\\
	Seien außerdem \[f(\nu, j) = \begin{cases}
		1, & i = \min\{j \mid \nu \in H_j\}\\
		0, & \text{ sonst}
	\end{cases}\] und $H = \{(\nu, j) \in U \mid f(\nu, j)= 1\}$. Es gilt $\left|H\right| = \left|G\right|$. Mit diesen Überlegungen kann nun ein Algorithmus formuliert werden.
	\begin{enumerate}
		\item $k=0$
		\item for $i=1$ to $N$
		\item wähle $(\nu, j) \in_R U$
		\item if $f(\nu, j) = 1$ then $k=k+1$
		\item output $\left|U\right| \frac{k}{N}$
	\end{enumerate}
	\begin{theorem}
		Für $N \geq \frac{4}{\varepsilon^2}m\ln\frac{2}{\delta}$ ist die Ungleichung in Satz \ref{thm: dnf sat ineq} erfüllt. Insbesondere ist die Größe von $N$ unabhängig von $\left|G\right|$.
	\end{theorem}
	Wieso können wir aber ein Stichprobe unter Gleichverteilung aus $U$ wählen? Dafür schreiben wir $\forall j$\[\left|H_j\right| = 2^{n-\#(\text{Literale in } H_j)}\]
	Damit kann der Algorithmus ein wenig umformuliert werden zu
	\begin{enumerate}
		\item $k=0$
		\item for $i=1$ to $N$
		\item wähle $j \in \{1,2,\dots,m\}$ mit Wahrscheinlichkeit $\frac{\left|H_j\right|}{\left|U\right|}$
		\item wähle $\nu \in_R H_j$
		\item if $f(\nu, j) = 1$ then $k=k+1$
		\item output $\left|U\right| \frac{k}{N}$
	\end{enumerate}
	Es folgt, dass $\mathbb{P}[(\nu,j)] = \frac{\left|H_j\right|}{\left|U\right|} \left|H_j\right| = \frac{1}{\left|U\right|}$.
	\begin{theorem}
		Sei $U$ eine endliche Menge und $\left|U\right|$ bekannt. Seien $H_1,H_2,\dots H_m \subseteq U$ nicht notwendigerweise disjunkte Teilmengen und \[H = \bigcup_{i=1}^m H_i\] Es gelte außerdem, dass \begin{itemize}
			\item $\left|H_i\right|$ in polynomialer Zeit berechenbar ist
			\item Man kann aus $H$ unter Gleichverteilung eine Stichprobe entnehmen
			\item $\nu \in H_i$ $\forall i$ kann effizient entschieden werden
		\end{itemize}
		Dann folgt: $\left|H\right|$ kann in polynomieller Zeit in $n, \frac{1}{\varepsilon}, \log \frac{1}{\delta}$ approximiert werden, wobei die Ungleichung aus Satz \ref{thm: dnf sat ineq} erfüllt ist.
	\end{theorem}
\section{Parametrisierte Algorithmen}
	\begin{definition}
		Ein parametrisiertes (Entscheidungs-) Problem ist ein Paar $(L,\Sigma)$ wobei \begin{enumerate}
			\item $L \subseteq \Sigma^* \times \Sigma^*$ oder
			\item $L \subseteq \Sigma^* \times \mathbb{N}$
		\end{enumerate}
	\end{definition}
	\begin{example}
		\label{bsp: FPT}
		\begin{enumerate}
			\item $L_1 = \{(G, k) \mid G \text{ hat ein Clique } \geq k\}$
			\item $L_2 = \{(G,k) \mid G \text{ hat Maximalgrad } \Delta(G) = k \text{ und ist planar}\}$
		\end{enumerate}
	\end{example}
	Für solche Probleme verwenden wir die Komplexitätsklasse FPT (Fixed Parameter Tractable). Solche Probleme sind algorithmisch lösbar in $\mathcal{O}(f(k)\cdot p(\left|x\right|))$ für ein Polynom $p$ und eine beliebige Funktion $f$ in $k$. $p$ ist unabhängig von $k$ und $f$ ist unabhängig von $n$. Für $L_1$ in Beispiel \ref{bsp: FPT} existiert ein Algorithmus mit Laufzeit $\mathcal{O}(n^kk^2)$, was die Definition für FPT nicht erfüllt. Tatsächlich ist unbekannt, ob $L_1\in$ FPT.
	\begin{remark}
		$P \subseteq FPT$.
	\end{remark}
	\subsection{Tiefenbeschränkte Suchbäume}
	Wir führen diese Strategie anhand des Vertex Cover Problems ein. Dieses Problem lässt sich schreiben als $\{(G,k) \mid G \text{ hat ein VC der Größe } \leq k\}$.\\
	Wir benötigen zuerst einen Suchbaum. Dieser besteht aus Knoten $(A,C)$ wobei $A$ ein Teilgraph von $G$ ist und $C \subseteq V$ mit $\left|C\right|\leq k$ eine Menge von Knoten ist. Die Strategie ist jetzt klar. Man berechnet alle möglichen Knotenmengen $C$ mit $\left|C\right|\leq k$ und dazugehörigen Teilgraphen $A = G-C$ und erhält ein Vertex Cover $C^*$, wenn ein Knoten $(A=(V_A,\varnothing), C^*)$ erreicht wird. Ansonsten existiert kein Vertex Cover der Größe $\leq k$.\\
	Der Suchbaum beginnt in der Wurzel $(G,\varnothing)$ und jeder Kindknoten entsteht aus seinem Elternknoten indem ein $v \in V_A$ entfernt und zu $C_A$ hinzugefügt wird. Dieser Baum hat maximale Tiefe $k$ und damit höchstens $2^k$ Knoten. Die Berechnung eines Knoten im Suchbaum benötigt Laufzeit $n$ (wird ein Knoten entfernt, so auch alle inzidenten Kanten, das sind $\leq n-1$ viele). Zum Überprüfen, ob eine Knotenmenge $C_A$ ein Vertex Cover impliziert, benötigt Laufzeit $m$ (es muss überprüft werden, ob in $A$ jede Kante überdeckt wird). Die Laufzeit ist demnach $\mathcal{O}(2^k(n+m))$ und damit ist $VC \in FPT$.\\
	
	Ein weiteres Problem, das sich mit dieser Technik lösen lässt, ist Hitting Set. Dieses Problem beruht auf einer endlichen Menge $S = \{a_1,a_2,...,a_n\}$ und einem Mengensystem $\mathcal{P}(S)\supseteq F = \{X_1,X_2,...,X_m\}$. Die Frage ist, ob es eine Menge $H \subseteq S$ mit $\left|H\right| \leq k$ gibt, sodass $H\cap X_i \neq \varnothing$ $\forall i \in [m]$. Dafür nehmen wir außerdem an, dass $\left|X_i\right| \leq d$ für alle $i$.\\
	Wir konstruieren wieder den Suchbaum. Die Knoten sind in diesem Fall $(F',H)$, wobei $F' = \{X_1',X_2',...,X_l'\} \subseteq F$ und $H \subseteq S$. Die Wurzel ist wieder $(F,\varnothing)$ und aus einem Knoten $(F',H)$ entstehen seine Kinder, indem ein Element $a' \in S\setminus H$ gewählt wird und $F'' = F' \setminus \{X_j \mid a' \in X_j\}$ und $H' = H \cup \{a'\}$ berechnet wird. Es existiert ein Hitting Set $H^*$ mit $\left|H^*\right|\leq k$ genau dann, wenn es einen Knoten $(\varnothing, H^*)$ gibt. Der Suchbaum hat höchstens $d^k$ Knoten und daher ein Laufzeit von $\mathcal{O}(d^k\cdot p(n,m))$.\\
	
	Als letztes Beispiel für diese Technik betrachten wir Independent Sets auf planaren Graphen. Wir benötigen dafür zwei einfache Resultate.
	\begin{lemma}
		In einem planaren Graphen $G$ mit $n$ Knoten, $m$ Kanten und $f$ Regionen gilt $n-m+f = 2$.
	\end{lemma}
	\begin{lemma}
		In einem planaren Graphen $G$ gibt es einen Knoten von Grad $\leq 5$.
	\end{lemma}
	Der Suchbaum besteht nun aus Knoten $(H,S)$ mit planaren Teilgraphen $H$ und $S \subseteq V$. Die Wurzel ist $(G,\varnothing)$. Für jeden inneren Knoten wählen wir ein $v \in V_H$ von Grad höchstens 5 und wir konstruieren die Kinder, indem entweder $v$ oder einer seiner Nachbarn in das IS hinzugefügt wird und dieser Knoten mit allen seinen Nachbarn aus $H$ entfernt wird. Es folgt daher, dass der Suchbaum Maximalgrad 6 hat. Gibt es ein IS der Größe mindestens $k$, so muss entweder $v$ oder einer seiner Nachbarn darin enthalten sein und daher ist eine der 6 Optionen richtig. In so einem Fall gibt es also einen Knoten $(\varnothing, S^*)$ im Suchbaum und $S^*$ ist in Independent Set der Größe $k$. Die Größe des Baumes ist beschränkt durch $6^k$, also erreicht man eine Laufzeit von $\mathcal{O}(6^k p(n))$.
\end{document}